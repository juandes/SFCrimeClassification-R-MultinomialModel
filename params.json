{"name":"San Francisco Crime Classification (Kaggle competition) using R and Logistic Regression","tagline":"Third attempt at the Kaggle competition \"San Francisco Crime Classification\"","body":"# San Francisco Crime Classification (Kaggle competition) using R and multinomial logistic regression via neural networks\r\n\r\n## Overview\r\n***\r\n\r\nThe \"San Francisco Crime Classification\" challenge, is a [Kaggle](https://www.kaggle.com) competition aimed to predict the category of the crimes that occurred in the city, given the time and location of the incident.\r\n\r\nIn this post, I explain and outline my third solution to this challenge. This time using R (again).\r\n\r\nLink to the competition: [San Francisco Crime Classification](https://www.kaggle.com/c/sf-crime)\r\n\r\n## Learning method\r\n***\r\n\r\nThe algorithm chosen for this solution, is a variation of [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression), a classification model based on regression where the dependent variable (what we want to predict) is categorical (opposite of continuous), implemented using [neural networks](https://stat.ethz.ch/R-manual/R-devel/library/nnet/html/multinom.html).\r\n\r\n## Data\r\n***\r\nThe competition provides two dataset: a train data set and a test dataset. The\r\ntrain dataset is made of 878049 observations and the test dataset, of 884262\r\nobservations.\r\n\r\nBoth of them contains incidents from January 1, 2003 to May 13, 2015.\r\n\r\n### Data fields\r\n\r\nThese are the features of the datasets:\r\n* Dates : timestamp of the crime incident.\r\n* Category: Category of the incident. Also, this is the variable we want to predict.\r\nThis variable is available only in the train dataset.\r\n* Descript: A short description of the incident. This variable is available only \r\nin the train dataset.\r\n* DayOfWeek: Day of the week where the incident occurred.\r\n* PdDistrict: Police Department District\r\n* Resolution: Outcome of the incident. This variable is available only in the \r\ntrain dataset.\r\n* Address: Address of the incident.\r\n* X: Longitude\r\n* Y: Latitude\r\n\r\n## Model development\r\n***\r\n\r\n#### Package installation and data loading\r\n\r\nFor this solution, I used the `nnet` package. To install it, simply run this command in R.\r\n\r\n```r\r\ninstall.packages('nnet')\r\n```\r\n\r\nOnce the package is downloaded , the next step is calling the library, followed by setting the working directory, and loading both datasets.\r\n\r\n```r\r\nlibrary(nnet)\r\nsetwd(\"~/path/to/working/directory\")\r\ntrain <- read.csv(\"~/path/to/working/directory/train.csv\")\r\ntest <- read.csv(\"~/path/to/working/directory/test.csv\")\r\n```\r\n\r\n#### Preparing the dataset\r\n\r\nAfter loading the datasets, the next thing I did was creating a new dataframe with the columns needed (Category, DayOfWeek and PdDistrict), this way we can save precious memory space.\r\n\r\n```r\r\n# New dataframes\r\ntrain.df <- data.frame(Category = train$Category, DayOfWeek = train$DayOfWeek,\r\n                       PdDistrict = train$PdDistrict)\r\ntest.df <- data.frame(DayOfWeek = test$DayOfWeek, PdDistrict = test$PdDistrict)\r\n```\r\n\r\nThe next step is to add a new feature, the hour of the incident, to both datasets. This is done by calling the function `strftime` on the original date to remove just the hour.\r\n\r\n```r\r\n# Create a new column with the hour of the incident\r\ntrain.df$Hour <- sapply(train$Dates, function(x) as.integer(strftime(x, format = \"%H\")))\r\ntest.df$Hour <- sapply(test$Dates, function(x) as.integer(strftime(x, format = \"%H\")))\r\n```\r\n\r\nRemove the original dataframes.\r\n\r\n```r\r\n# Remove the original dataframes\r\nrm(train)\r\nrm(test)\r\n```\r\n\r\n#### Create and train the model.\r\n\r\nAfter pre-processing the data, the next step is to create and train the model. The model will predict the category of the crime using the day of the week when the incident occurred, the district where it occurred and the hour when it occurred, as the predictors.\r\n\r\nInstead of the default value of 100 iterations, I changed it to 500. Keep in mind the model will take some time (around 45 mins in my set up) to finish the training.\r\n\r\n```r\r\n# Multinomial log-linear model using the day of the week,  the district of the crime\r\n# and the hour of the incident as the predictors.\r\nmultinom.model <- multinom(Category ~ DayOfWeek + PdDistrict + Hour, data = train.df, \r\n                 maxit = 500)\r\n```\r\n\r\n\r\n#### Prediction phase\r\n\r\nNow we predict!\r\n\r\n```r\r\npredictions <- predict(multinom.model, test.df, \"probs\")\r\n```\r\n\r\n#### Preparing the submission file\r\n\r\nTo get a smaller file, I rounded down the number of digits.\r\n\r\n```r\r\nsubmission <- format(predictions, digits=4, scientific = FALSE)\r\nsubmission <- cbind(id = 0:884261, submission)\r\nsubmission <- as.data.frame(submission)\r\nwrite.csv(submission, file = \"results.csv\", row.names = FALSE)\r\n```\r\n\r\n## Conclusion\r\n***\r\n\r\nThe score received this time was way better than my previous attempts. First, I got a score of 26.74064, followed by 26.78360.\r\nThis time my score was 2.60502, which is a huge improvement. At the moment of writing, I do not have plans on improving the score, since I would like to tackle more advanced challenges.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}